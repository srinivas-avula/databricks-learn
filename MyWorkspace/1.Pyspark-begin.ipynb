{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06328884-4e70-46f2-8cfd-b30a9a90347f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setting up spark environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2121a4b0-bd9f-4a45-bb4d-fcc67244e0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Apache Spark is an open-source, distributed computing framework designed for fast big data processing. It simplifies working with massive datasets by providing easy-to-use APIs, in-memory computation, and support for multiple languages like Python, Scala, Java, and R.\n",
    "\n",
    "### What is Apache Spark?\n",
    "\n",
    "Definition: A cluster computing framework for large-scale data processing.\n",
    "\n",
    "Purpose: Handles big data workloads faster than traditional systems like Hadoop MapReduce.\n",
    "\n",
    "Key Feature: In-memory computation, which reduces disk I/O and speeds up processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002123d1-017e-42bb-a587-0128ee6dd266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Architecture (Step by Step)\n",
    "**Driver Program** : The main application that defines transformations and actions on data.   \n",
    "Coordinates tasks across the cluster.\n",
    "\n",
    "**Cluster Manager**: Allocates resources across machines (examples: YARN, Mesos, or Sparkâ€™s built-in manager).\n",
    "\n",
    "**Executors**: Worker processes running on cluster nodes.\n",
    "Execute tasks assigned by the driver.\n",
    "\n",
    "**Resilient Distributed Dataset (RDD)**: Core data structure in Spark.   \n",
    "Immutable, distributed collection of objects that can be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1162e758-8f97-49f0-80fc-0e2d2b5bbea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Start a Spark Session\n",
    "* Every PySpark program begins with a SparkSession (the entry point to Spark).\n",
    "* appName: Name of your application.\n",
    "* getOrCreate(): Creates a new session or reuses an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0faa8946-7efd-46df-8fc9-8443f4811b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3896f6-4e02-45cd-a3f9-6977ace68ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Create a Simple Dataset\n",
    "You can create a DataFrame directly from Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30af1d45-731f-4b51-a2d3-17905761c108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1, 'Alice'), (2, 'Bob'), (3, 'Charlie')]\n",
    "columns = ['id', 'name']\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e22274-3780-42b5-94ac-2c4882f3d173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Perform Basic Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a756fd-e96c-4cd1-8169-51a46b27b840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific column\n",
    "dataframe.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a623ef-8d1f-4aa3-b0d6-3717430a249d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows\n",
    "dataframe.filter(dataframe.id > 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cee2b2-9c1c-4e01-82bf-b54e6787faf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Always stop the Spark session when done.\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.Pyspark-begin",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
